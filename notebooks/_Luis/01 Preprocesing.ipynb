{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c107bb-1281-42a8-a54d-de27f86ba73d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "from EDA.ValidationData import Validation_data, DataTypeAnalysis\n",
    "from EDA.StatisticalAnalysis import StatisticalAnalysis\n",
    "from EDA.PlotGeometryAnalysis import PlotGeometryAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60c9cc5-0066-4f3b-8724-a3742dfd33dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preprocesing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06654f33-4d4c-4363-8e74-8558a54026bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Leemos los datos de PROCEESED la tabla Delta\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/proyectocongestion_presentation/tablacaracteristicas_congestion_tabladelta\")\n",
    "datos = df_delta.toPandas()\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ddeb8c2-cc32-4823-8deb-a372424fa03d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mask = (datos['x'] == 0) & (datos['y'] == 0) & (datos['z'] == 0)\n",
    "datos[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e861bf-b4f6-4bd1-ad2b-5a50d86751aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Hacer el Balanceo de datos import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    " \n",
    "# Convertimos a index la fecha\n",
    "datos = datos.set_index('instant_date_t')\n",
    "\n",
    "# Eliminamos registros con coordenadas (0,0,0)\n",
    "mask = (datos['x']>0) & (datos['y']>0) & (datos['z']>0)\n",
    "datos = datos[mask]\n",
    "\n",
    "# Eliminacion de variables tipo object\n",
    "datos = datos.drop(columns=['nombre', 'nombre_equipo'])\n",
    "\n",
    "\n",
    "# Filtrar las fechas donde congestion == 1\n",
    "fechas_congestion_1 = datos[datos['congestion'] == 1].index\n",
    " \n",
    "# Generar un rango de fechas completo dentro del rango del DataFrame\n",
    "fecha_min = datos.index.min()\n",
    "fecha_max = datos.index.max()\n",
    "rango_completo = pd.date_range(start=fecha_min, end=fecha_max, freq='S')\n",
    " \n",
    "# Excluir las fechas de congestion == 1 del rango completo\n",
    "fechas_disponibles = rango_completo.difference(fechas_congestion_1)\n",
    " \n",
    "# Aplicar SMOTE para generar nuevas muestras de la clase minoritaria (congestion == 0)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    " \n",
    "# Usar todas las columnas como características para SMOTE excepto 'congestion'\n",
    "X = datos.drop(columns=['congestion'])\n",
    "y = datos['congestion']\n",
    " \n",
    "# Convertir las fechas del índice a números para usarlas con SMOTE\n",
    "X['instant_date_num'] = X.index.astype(int) / 10**9  # Convertir a segundos\n",
    " \n",
    "# Aplicar SMOTE\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    " \n",
    "# Convertir de nuevo a DataFrame\n",
    "X_res = pd.DataFrame(X_res, columns=X.columns)\n",
    "y_res = pd.Series(y_res, name='congestion')\n",
    " \n",
    "# Filtrar las nuevas muestras generadas por SMOTE (la diferencia entre las originales y las nuevas)\n",
    "nuevas_muestras = X_res[len(datos):]\n",
    "nuevas_congestion = y_res[len(datos):]\n",
    " \n",
    "# Asignar nuevas fechas secuenciales de fechas_disponibles a las nuevas muestras\n",
    "nuevas_fechas_asignadas = fechas_disponibles[:len(nuevas_muestras)]\n",
    "nuevas_muestras['instant_date'] = nuevas_fechas_asignadas\n",
    " \n",
    "# Crear el DataFrame final combinando los datos originales y las nuevas muestras\n",
    "nuevas_muestras = nuevas_muestras.set_index('instant_date')\n",
    "nuevas_muestras = nuevas_muestras.drop(columns=['instant_date_num'])\n",
    "nuevas_muestras['congestion'] = nuevas_congestion.values\n",
    " \n",
    "# Combinar los datos originales y las nuevas muestras\n",
    "datos_cong_balanceado = pd.concat([datos, nuevas_muestras]).sort_index()\n",
    "\n",
    "datos_cong_balanceado.index.name = 'instant_date_t'\n",
    "#datos_cong_balanceado = datos_cong_balanceado.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3cff42e-d1ef-40d1-a6dd-efea23504d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mask = (datos_cong_balanceado['x'] == 0) & (datos_cong_balanceado['y'] == 0) & (datos_cong_balanceado['z'] == 0)\n",
    "datos_cong_balanceado[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87902be7-3565-4b25-b939-da6584133058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Convertir el DataFrame de Pandas a un DataFrame de Spark\n",
    "spark_datos = spark.createDataFrame(datos_cong_balanceado)\n",
    " \n",
    "# 4. Guardar los datos preprocesados en una tabla Delta en el Azure Storage\n",
    "# Asegurar que las columnas de fecha y hora mantengan sus tipos de datos\n",
    "#spark_datos = spark_datos.withColumn(\"Event_Date\", col(\"Event_Date\").cast(\"timestamp\"))\n",
    "#spark_datos = spark_datos.withColumn(\"instant_date_t\", col(\"instant_date_t\").cast(\"timestamp\"))\n",
    " \n",
    "# Nombre de la tabla Delta a guardar\n",
    "nombre_tabla_delta = \"dbproyectocongestion_presentation.tablacaracteristicas_congestion_tabladelta_2\"\n",
    " \n",
    "# 4.1 Verificar si ya existe la tabla Delta\n",
    "if spark.catalog.tableExists(nombre_tabla_delta):\n",
    "    # Eliminar la tabla Delta existente\n",
    "    spark.sql(\"DROP TABLE IF EXISTS \" + nombre_tabla_delta)\n",
    " \n",
    "# 4.2 Guardar los datos preprocesados en una tabla Delta\n",
    "spark_datos.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(nombre_tabla_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5bb1b6-0828-4b69-91e0-10798783d10f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "015ab866-4223-4323-b73b-58a0fdfc87d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1761a709-1579-4b62-a6fe-6aedb45173d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad31f6c-1767-458f-96e3-b2a5ccbb5dde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Leer Datos Sin Balancear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a4134b-3bfb-4842-97e4-1f1481554f86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "from EDA.ValidationData import Validation_data, DataTypeAnalysis\n",
    "from EDA.StatisticalAnalysis import StatisticalAnalysis\n",
    "from EDA.PlotGeometryAnalysis import PlotGeometryAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f0530a-eec0-47e8-9873-92d4f899ffb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#1. Leemos los datos de PROCEESED la tabla Delta \n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/processed/proyectocongestion_processed/datapreprocessed_congestion_tabladelta\")\n",
    "datos_sin_balanceo = df_delta.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9cd71eb-3b9f-4558-a0fe-196d7a076f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Limpieza de variables No prioritarias\n",
    "columnas_a_eliminar = ['nombre_equipo','nombre']\n",
    "\n",
    "# 2.1 Filtrar las columnas que existen en el DataFrame\n",
    "columnas_existentes = [col for col in columnas_a_eliminar if col in datos_sin_balanceo.columns]\n",
    "\n",
    "# 2.2 Verificar si hay columnas para eliminar\n",
    "if columnas_existentes:\n",
    "    datos_sin_balanceo.drop(columnas_existentes, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784a3c44-3fad-4213-b4b2-b2e020cb207b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtramos los valores de x, y, z que son 0s\n",
    "mask = (datos_sin_balanceo['x'] == 0) & (datos_sin_balanceo['y'] == 0) & (datos_sin_balanceo['z'] == 0)\n",
    "datos_sin_balanceo = datos_sin_balanceo[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc19b0b-a635-4ab6-944b-4855a63f3175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_sin_balanceo['congestion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda4be62-62ee-4f5e-abcd-84c7f9840428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Limpieza de variables No prioritarias\n",
    "columnas_a_eliminar = ['nombre_equipo','nombre','start_time_alert','end_time_alert','Event_Date']\n",
    "\n",
    "# 2.1 Filtrar las columnas que existen en el DataFrame\n",
    "columnas_existentes = [col for col in columnas_a_eliminar if col in datos_sin_balanceo.columns]\n",
    "\n",
    "# 2.2 Verificar si hay columnas para eliminar\n",
    "if columnas_existentes:\n",
    "    datos_sin_balanceo.drop(columnas_existentes, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23de717-0f17-468f-aaec-b4fbea40faaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos = datos_sin_balanceo.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "045ec43f-068f-4a51-9b56-3d75fec5162f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hacemos el Balanceo de la serie temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bd1677-0008-40b3-bc29-0c6c2f4623af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Convertimos a index la fecha\n",
    "datos = datos.set_index('instant_date_t')\n",
    "\n",
    "# Filtrar las fechas donde congestion == 1\n",
    "fechas_congestion_1 = datos[datos['congestion'] == 1].index\n",
    "\n",
    "# Generar un rango de fechas completo dentro del rango del DataFrame\n",
    "fecha_min = datos.index.min()\n",
    "fecha_max = datos.index.max()\n",
    "rango_completo = pd.date_range(start=fecha_min, end=fecha_max, freq='S')\n",
    "\n",
    "# Excluir las fechas de congestion == 1 del rango completo\n",
    "fechas_disponibles = rango_completo.difference(fechas_congestion_1)\n",
    "\n",
    "# Aplicar SMOTE para generar nuevas muestras de la clase minoritaria (congestion == 0)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Usar todas las columnas como características para SMOTE excepto 'congestion'\n",
    "X = datos.drop(columns=['congestion'])\n",
    "y = datos['congestion']\n",
    "\n",
    "# Convertir las fechas del índice a números para usarlas con SMOTE\n",
    "X['instant_date_num'] = X.index.astype(int) / 10**9  # Convertir a segundos\n",
    "\n",
    "# Aplicar SMOTE\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Convertir de nuevo a DataFrame\n",
    "X_res = pd.DataFrame(X_res, columns=X.columns)\n",
    "y_res = pd.Series(y_res, name='congestion')\n",
    "\n",
    "# Filtrar las nuevas muestras generadas por SMOTE (la diferencia entre las originales y las nuevas)\n",
    "nuevas_muestras = X_res[len(datos):]\n",
    "nuevas_congestion = y_res[len(datos):]\n",
    "\n",
    "# Asignar nuevas fechas secuenciales de fechas_disponibles a las nuevas muestras\n",
    "nuevas_fechas_asignadas = fechas_disponibles[:len(nuevas_muestras)]\n",
    "nuevas_muestras['instant_date'] = nuevas_fechas_asignadas\n",
    "\n",
    "# Crear el DataFrame final combinando los datos originales y las nuevas muestras\n",
    "nuevas_muestras = nuevas_muestras.set_index('instant_date')\n",
    "nuevas_muestras = nuevas_muestras.drop(columns=['instant_date_num'])\n",
    "nuevas_muestras['congestion'] = nuevas_congestion.values\n",
    "\n",
    "# Combinar los datos originales y las nuevas muestras\n",
    "datos_balanceados = pd.concat([datos, nuevas_muestras]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2bb99b-12ed-4d96-8ae5-2ccd148f3a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_balanceados = datos_balanceados.reset_index(names='instant_date_t')\n",
    "\n",
    "# 3. Convertir el DataFrame de Pandas a un DataFrame de Spark\n",
    "spark_datos = spark.createDataFrame(datos_cong_balanceado)\n",
    " \n",
    "# 4. Guardar los datos preprocesados en una tabla Delta en el Azure Storage\n",
    "# Asegurar que las columnas de fecha y hora mantengan sus tipos de datos\n",
    "#spark_datos = spark_datos.withColumn(\"Event_Date\", col(\"Event_Date\").cast(\"timestamp\"))\n",
    "#spark_datos = spark_datos.withColumn(\"instant_date_t\", col(\"instant_date_t\").cast(\"timestamp\"))\n",
    " \n",
    "# Nombre de la tabla Delta a guardar\n",
    "nombre_tabla_delta = \"dbproyectocongestion_presentation.tablacaracteristicas_congestion_tabladelta_v3\"\n",
    " \n",
    "# 4.1 Verificar si ya existe la tabla Delta\n",
    "if spark.catalog.tableExists(nombre_tabla_delta):\n",
    "    # Eliminar la tabla Delta existente\n",
    "    spark.sql(\"DROP TABLE IF EXISTS \" + nombre_tabla_delta)\n",
    " \n",
    "# 4.2 Guardar los datos preprocesados en una tabla Delta\n",
    "spark_datos.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(nombre_tabla_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b83c4b-68fa-4e47-9835-bf85093e6531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 Preprocesing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
