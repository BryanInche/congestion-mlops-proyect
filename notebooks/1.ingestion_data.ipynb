{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b466958e-22fd-4698-b677-b0ebb1f68426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias que se usaran\n",
    "import pandas as pd  # Libreria para administrar tablas, y realizar trabajos con distintas formas de tablas o dataframes\n",
    "import numpy as np   # Libreria para poder hacer operaciones matematicas y matriciales\n",
    "import matplotlib.pyplot as plt # Libreria para realizar graficos \n",
    "from tabulate import tabulate   # Permite formatear y mostrar de mejor manera los datos tabulares\n",
    "import seaborn as sns  # Libreria para realizar graficos y vizualizaciones\n",
    "import psycopg2        # Libreria que permite la conexion con PostgresSQL\n",
    "from matplotlib.backends.backend_pdf import PdfPages # Libreria que permite exportar graficos en pdf\n",
    "\n",
    "#Configuramos pandas para que podamos vizualizar todas las columnas y filas la estadistica descriptiva de todas las variables\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "#Configuramos pandas para que lanze valores con una precision de hasta 6 decimales\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "defdcc17-4cb5-42a7-a10d-ff8b8accfaef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Extraemos los datos del C4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80354573-4321-4c13-b594-746abd89d8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Información de la conexión a PostgreSQL\n",
    "host = \"srv-postgres-d4m.postgres.database.azure.com\"\n",
    "database = \"ControlSenseDB\"\n",
    "user = \"administrador\"\n",
    "password = \"Protobuffers2024\"\n",
    "\n",
    "# Establecer la conexión a la base de datos\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Crear un cursor para ejecutar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Establecer la zona horaria antes de ejecutar la consulta\n",
    "    time_zone_query = \"SET TIME ZONE 'America/Lima';\"\n",
    "    cursor.execute(time_zone_query)\n",
    "\n",
    "    # Tu consulta SQL equipos de acarreo a considerar en c4mequipos y datacamion: 61,63,64,65,68,69,71,74,103,122,125,126,198,199,201,204,205\n",
    "    # equipos acarreo disponibles en telemetria h4m(2dias) : 61,65,68,126,199,201,*204*,205\n",
    "    # Query de Join entre DATACAMION Y TSEQUIPOS\n",
    "    tu_query_sql = '''\n",
    "    select main.*, e.id_equipo, e.nombre from (SELECT *\n",
    "    FROM public.getsensorsvaluesmod(201, '2024-02-27 07:00:30.612-05', '2024-03-03 06:27:30.612-05') a --data_camion\n",
    "    left join tp_tramosstat b\n",
    "    on b.id = (select id from tp_tramosstat where id_tptramosstat = a.tramosidsnew_t and tiem_creac < a.instant_date_t\n",
    "                    order by tiem_creac desc limit 1)) main\n",
    "    left join ts_equipos e\n",
    "    on main.eq_id = e.id_equipo\t\t\t\t\n",
    "    where main.nombre_tramo is not null\n",
    "    order by main.instant_date_t asc\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(tu_query_sql)\n",
    "\n",
    "    # Obtener los resultados en un DataFrame de pandas\n",
    "    resultados17 = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "    # Cerrar el cursor y la conexión\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Hacer lo que necesites con los resultados\n",
    "    print(resultados17.head())\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error al conectar a la base de datos PostgreSQL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87692726-bc7e-4368-b32a-9a3c2ae9a6d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ff35ec-047f-4c69-8ccd-d3b5d5165ffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados17.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dab1b61-2df8-4365-a40d-d5b0d817c6e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Guardarmos directamente el dfpandas en un csv en el BlobStorage\n",
    "Nota importante: Debes comentar la variable \"connection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1931775-096f-4956-b03f-3700643b48c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias para usar el Azure DataLake y DataBrinks\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# 1.Guardamos el df convertido en csv en la variable csv_data\n",
    "csv_data = resultados17.to_csv(index=False)\n",
    "\n",
    "# 2. Obtener conection Azure DataLake,interfaz de Azure(Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# 3. Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# 4. Identificamos el nombre del contenedor(container_name) y nombre del archivo(definir blob_name) en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "blob_name = \"datos_raw_shougang_equipo201.csv\"\n",
    "\n",
    "# Subir el archivo CSV al Blob Storage\n",
    "# with open(\"datos_raw_shougang_equipo61.csv\", \"rb\") as data:\n",
    "#     blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "#     blob_client.upload_blob(data)\n",
    "\n",
    "# 5. Guardamos el archivo CSV al Blob Storage\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "blob_client.upload_blob(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f336d3d-f0e0-4423-b0e5-831dbbcb1030",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.2 Unir todos los csv(datalake-BlobStorage), Se leen varios dfs(csv) se Unen en 1 solo\n",
    "Nota importante: Debes comentar la variable \"conection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5971fe-516f-423c-a710-06ba58861b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias para usar el Azure DataLake y DataBrinks\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# 2. Obtener conection Azure DataLake,interfaz de Azure(Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# 3. Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# 4. Identificamos el nombre del contenedor(container_name)\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "\n",
    "# 5. Se crea una lista para almacenar los DataFrames de cada archivo CSV\n",
    "dfs = []\n",
    "\n",
    "# 5.1 Nombres de los archivos CSV almacenados en el Contenedor de BlobStorage\n",
    "csv_files = [\"datos_raw_shougang_equipo61.csv\",\"datos_raw_shougang_equipo199.csv\", \"datos_raw_shougang_equipo201.csv\"]\n",
    "\n",
    "# # Leer cada archivo CSV y guardarlos en DataFrames individuales\n",
    "# for csv_file in csv_files:\n",
    "#     # Descargar el archivo CSV del Blob Storage\n",
    "#     with open(csv_file, \"wb\") as my_blob:\n",
    "#         blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_file)\n",
    "#         blob_data = blob_client.download_blob()\n",
    "#         blob_data.readinto(my_blob)\n",
    "    \n",
    "#     # Leer el archivo CSV en un DataFrame de Pandas\n",
    "#     df = pd.read_csv(csv_file)\n",
    "    \n",
    "#     # Agregar el DataFrame a la lista\n",
    "#     dfs.append(df)\n",
    "\n",
    "# 6. Leemos cada archivo CSV y guardarlos en DataFrames individuales\n",
    "for csv_file in csv_files:\n",
    "    # Obtener el blob del Blob Storage\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_file)\n",
    "    \n",
    "    # Descargar el contenido del blob\n",
    "    blob_data = blob_client.download_blob()\n",
    "    \n",
    "    # Leer el contenido del blob en un objeto BytesIO\n",
    "    bytes_io = BytesIO()\n",
    "    blob_data.download_to_stream(bytes_io)\n",
    "    bytes_io.seek(0)  # Asegurar que la posición del cursor esté al inicio del archivo\n",
    "    \n",
    "    # Leer el archivo CSV en un DataFrame de Pandas desde el objeto BytesIO\n",
    "    df = pd.read_csv(bytes_io)\n",
    "    \n",
    "    # Agregar el DataFrame a la lista\n",
    "    dfs.append(df)\n",
    "\n",
    "# 7. Se Concatena todos los DataFrames en uno solo df\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Visualizar el DataFrame resultante\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0617df91-9293-408a-afda-68d52bc35de3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e95de91-8c33-456c-8670-51a1c5f64ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df['nombre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4013045c-f6c0-48e2-bda4-8cdf56b43d43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Guardar el csv consolidado C4M en el BlobStorage \n",
    "Nota importante: Debes comentar la variable \"conection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfa8984-f4b1-4391-95bd-9bee20c21395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias para usar el Azure DataLake y DataBrinks\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# 1.Guardamos el df convertido en csv en la variable csv_data\n",
    "datos_total = result_df.to_csv(index=False)\n",
    "\n",
    "# 2. Obtener conection Azure DataLake,interfaz de Azure(Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# 3. Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# 4. Identificamos el nombre del contenedor(container_name) y nombre del archivo(definir blob_name) en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "blob_name = \"datos_raw_shougang_c4m.csv\"\n",
    "\n",
    "# 5. Guardar el archivo CSV al Blob Storage\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "blob_client.upload_blob(datos_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab4151fc-2e02-49ce-9a32-ea4c8f0ad667",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 Leer csv consolidado c4m desde el BlobStorage\n",
    "Nota importante: Debes comentar la variable \"conection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3a6b2b-a765-4615-a84a-3d673c2363e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias para usar el Azure DataLake y DataBrinks\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import io\n",
    "\n",
    "# 2. Obtener conection Azure DataLake,interfaz de Azure(Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# 3. Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# 4. Identificamos el nombre del contenedor(container_name) y nombre del archivo(definir blob_name) en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "blob_name = \"datos_raw_shougang_c4m.csv\"\n",
    "\n",
    "# 5. Obtener el blob_client\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "# 6. Leer el contenido del blob como texto\n",
    "blob_data = blob_client.download_blob().content_as_text()\n",
    "\n",
    "# 7. Leer el archivo CSV en un DataFrame de Pandas desde el texto\n",
    "datos_c4m = pd.read_csv(io.StringIO(blob_data))\n",
    "datos_c4m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a918ea2c-b410-42e3-aed7-500ebb980fbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec0df6a-3a1f-401a-8390-59c3ed554f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m[\"eq_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8b3904-307d-41a7-a9be-19f4cadbb178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m['id_equipo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdb6d27-7336-400c-818b-7e6a15f82648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m['nombre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3c1c4f-6e37-43e1-bc44-0d4357d3a5cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Cargamos datos del H4M alojados en el BlobStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9d1eda-ad98-4213-9f6d-1af13e9bfa65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias para usar el Azure DataLake y DataBrinks\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# 2. Obtener conection Azure DataLake,interfaz de Azure(Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# 3. Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# 4. Identificar el Nombre del contenedor en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_h4m/operacion_shougang/equipo_fc98/\"\n",
    "\n",
    "# 5. Lista para almacenar los DataFrames de cada archivo CSV\n",
    "dfs = []\n",
    "\n",
    "# 5.1 Nombres de los archivos CSV\n",
    "csv_files = [\"fc98_allvar_00_7_1marzo.csv\", \"fc98_allvar_7_13_1marzo.csv\",\"fc98_allvar_13_19_1marzo.csv\",\n",
    "             \"fc98_allvar_19_2359_1marzo.csv\",\"fc98_allvar_00_2359_2marzo.csv\"]\n",
    "\n",
    "# 6. Leer cada archivo CSV y guardarlos en DataFrames individuales\n",
    "for csv_file in csv_files:\n",
    "    # Obtener el blob del Blob Storage\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_file)\n",
    "    \n",
    "    # Descargar el contenido del blob\n",
    "    blob_data = blob_client.download_blob()\n",
    "    \n",
    "    # Leer el contenido del blob en un objeto BytesIO\n",
    "    bytes_io = BytesIO()\n",
    "    blob_data.download_to_stream(bytes_io)\n",
    "    bytes_io.seek(0)  # Asegurar que la posición del cursor esté al inicio del archivo\n",
    "    \n",
    "    # Leer el archivo CSV en un DataFrame de Pandas desde el objeto BytesIO\n",
    "    df = pd.read_csv(bytes_io)\n",
    "    \n",
    "    # Agregar el DataFrame a la lista\n",
    "    dfs.append(df)\n",
    "\n",
    "# 7. Concatenar todos los DataFrames en uno solo\n",
    "result_h4m_fc98 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 8. Visualizar el DataFrame resultante\n",
    "result_h4m_fc98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1501e294-8b50-4a6f-b0cd-62a7b67262e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_h4m_fc89.shape, result_h4m_fc98.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f602637-f660-4644-b856-fcdf18fa3ce5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.1 Se unen los df de los equipos extraidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7b117d-d70a-49c9-a197-9593b5e8ef91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unir los dos DataFrames por filas\n",
    "resultado_h4m_allequipos = pd.concat([result_h4m_fc89, result_h4m_fc98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d2747d-0532-4901-bb54-ddec6ac9bd7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_h4m_allequipos.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8669e822-9664-4194-857d-e7feb48d1dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_h4m_allequipos['Equipment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ae7a090-8a43-4900-a347-5ecd0fc26ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1 Extraer datos de TP_EQUIPOS para extraer el Id_equipo y cruzar con H4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe59738b-7c8f-451c-af1e-914f4e85106f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Información de la conexión a PostgreSQL\n",
    "host = \"srv-postgres-d4m.postgres.database.azure.com\"\n",
    "database = \"ControlSenseDB\"\n",
    "user = \"administrador\"\n",
    "password = \"Protobuffers2024\"\n",
    "\n",
    "# Establecer la conexión a la base de datos\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Crear un cursor para ejecutar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Establecer la zona horaria antes de ejecutar la consulta\n",
    "    time_zone_query = \"SET TIME ZONE 'America/Lima';\"\n",
    "    cursor.execute(time_zone_query)\n",
    "\n",
    "    # Tu consulta SQL equipos de acarreo a considerar en c4mequipos y datacamion: 61,63,64,65,68,69,71,74,103,122,125,126,198,199,201,204,205\n",
    "    # equipos acarreo disponibles en telemetria h4m(2dias) : 61,65,68,126,199,201,*204*,205\n",
    "    tu_query_sql = '''\n",
    "    select * from ts_equipos\n",
    "    --where nombre = 'FC89'\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(tu_query_sql)\n",
    "\n",
    "    # Obtener los resultados en un DataFrame de pandas\n",
    "    resultados_equipos = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "    # Cerrar el cursor y la conexión\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Hacer lo que necesites con los resultados\n",
    "    print(resultados_equipos.head())\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error al conectar a la base de datos PostgreSQL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e780dbc9-cfed-4a43-af89-2d6fdbca7553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados_equipos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bfa3024-0c76-42cb-a0e4-84cdc53711bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2 Datos de H4M join TSEQUIPOS(C4M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e257ce4-07a8-4c0c-9dd1-83116ca370b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizar left join \n",
    "result_h4m = pd.merge(resultado_h4m_allequipos, resultados_equipos[['nombre','id_equipo']], left_on='Equipment', right_on='nombre', how='left')\n",
    "result_h4m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586044e7-9885-49d4-a515-39c88f8fed20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_h4m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f05dd6e4-46ac-4e65-a54d-e3dfd9b8e241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Preprocesamiento de Datos (Formato de Fechas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e88eab3-c59a-4cbb-9408-8d488837030b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df514378-290d-4d13-8812-9d376b76233d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ad941b-d974-49d4-894f-797890bdad31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_h4m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa69b8f9-3685-45d5-956c-e1f1fb45d7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_h4m.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5f34a3-f7f6-4832-ac94-4cc669cf6b75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4.1. Cambiamos a formato DateTime (Formato de Fechas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da87250-ebea-45a9-9197-2e861c81dd82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# datos_c4m['instant_date_t'] = pd.to_datetime(datos_c4m['instant_date_t'], format='%Y-%m-%d %H:%M:%S')\n",
    "# result_h4m['Event Date'] = pd.to_datetime(result_h4m['Event Date'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0054faa0-b8e5-4d37-b850-01f4c375b2d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Convertir a formato datetime, pero quitando la Zona horaria, y solo quedandonos hasta la parte de Segundos\n",
    "datos_c4m['instant_date_t'] = pd.to_datetime(datos_c4m['instant_date_t'], errors='coerce', utc=False).dt.tz_localize(None)\n",
    "\n",
    "#Convertir a formato datetime\n",
    "result_h4m['Event Date'] = pd.to_datetime(result_h4m['Event Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8622be07-5b42-4f57-ab63-48adbe0e0940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.2 Ver los resultados al cambiar de formato a datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b7d8cb-8136-4130-aacd-f3445e23aa6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_c4m['instant_date_t'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acedb552-5e80-4d4a-a60b-31cb93da9d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_h4m['Event Date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd98e74-4221-4aa4-9dd5-385f664ca7db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.3 Realizar el join especificando las columnas clave en cada DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47793499-1ccb-453b-87bb-2758e0535c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizar el join especificando las columnas clave en cada DataFrame\n",
    "resultado_c4m_h4m = pd.merge(datos_c4m, result_h4m, left_on=['eq_id', 'instant_date_t', 'nombre'], right_on=['id_equipo', 'Event Date', 'Equipment'], how='left')\n",
    "\n",
    "# resultado_join contendrá el DataFrame resultante después del join\n",
    "resultado_c4m_h4m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44819078-c85e-4df4-8cc7-059652fb73ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_c4m_h4m_filtrado = resultado_c4m_h4m[resultado_c4m_h4m['Event Date'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ba5f87-cc0a-4e4e-af68-18f2d4357625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_c4m_h4m_filtrado[[\"eq_id\",\"nombre_x\",\"instant_date_t\",\"Event Date\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807ca68a-9157-4a5e-8f65-bd553608cff2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_c4m_h4m_filtrado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2324dc-a683-407d-b527-d3d3934b1ae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultado_c4m_h4m_filtrado['eq_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c154f6ff-1444-4d50-ba39-4e774cffc9bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5. Guardamos la Tabla Consolidada C4M-H4M en el BlobStorage como csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a8d2c6-2489-408d-92dd-92b2e50b53ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias para usar el Azure DataLake y DataBrinks\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# 1.Guardamos el df convertido en csv en la variable csv_data\n",
    "datos_total_c4m_h4m = resultado_c4m_h4m_filtrado.to_csv(index=False)\n",
    "\n",
    "# 2. Obtener conection Azure DataLake,interfaz de Azure(Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# 3. Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# 4. Identificamos el nombre del contenedor(container_name) y nombre del archivo(definir blob_name) en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_consolidado/\"\n",
    "blob_name = \"datos_raw_consolidado.csv\"\n",
    "\n",
    "# 5. Guardar el archivo CSV al Blob Storage\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "blob_client.upload_blob(datos_total_c4m_h4m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c23975dc-3953-4766-ac04-36c1170505dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.ingestion_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
