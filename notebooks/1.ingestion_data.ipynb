{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b466958e-22fd-4698-b677-b0ebb1f68426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Librerias que se usaran\n",
    "import pandas as pd  # Libreria para administrar tablas, y realizar trabajos con distintas formas de tablas o dataframes\n",
    "import numpy as np   # Libreria para poder hacer operaciones matematicas y matriciales\n",
    "import matplotlib.pyplot as plt # Libreria para realizar graficos \n",
    "from tabulate import tabulate   # Permite formatear y mostrar de mejor manera los datos tabulares\n",
    "import seaborn as sns  # Libreria para realizar graficos y vizualizaciones\n",
    "import psycopg2        # Libreria que permite la conexion con PostgresSQL\n",
    "from matplotlib.backends.backend_pdf import PdfPages # Libreria que permite exportar graficos en pdf\n",
    "\n",
    "#Configuramos pandas para que podamos vizualizar todas las columnas y filas la estadistica descriptiva de todas las variables\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "#Configuramos pandas para que lanze valores con una precision de hasta 6 decimales\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "defdcc17-4cb5-42a7-a10d-ff8b8accfaef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Extraemos los datos del C4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80354573-4321-4c13-b594-746abd89d8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Información de la conexión a PostgreSQL\n",
    "host = \"srv-postgres-d4m.postgres.database.azure.com\"\n",
    "database = \"ControlSenseDB\"\n",
    "user = \"administrador\"\n",
    "password = \"Protobuffers2024\"\n",
    "\n",
    "# Establecer la conexión a la base de datos\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Crear un cursor para ejecutar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Establecer la zona horaria antes de ejecutar la consulta\n",
    "    time_zone_query = \"SET TIME ZONE 'America/Lima';\"\n",
    "    cursor.execute(time_zone_query)\n",
    "\n",
    "    # Tu consulta SQL equipos de acarreo a considerar en c4mequipos y datacamion: 61,63,64,65,68,69,71,74,103,122,125,126,198,199,201,204,205\n",
    "    # equipos acarreo disponibles en telemetria h4m(2dias) : 61,65,68,126,199,201,*204*,205\n",
    "    tu_query_sql = '''\n",
    "    select main.*, e.id_equipo, e.nombre from (SELECT *\n",
    "    FROM public.getsensorsvaluesmod(201, '2024-02-27 07:00:30.612-05', '2024-03-03 06:27:30.612-05') a --data_camion\n",
    "    left join tp_tramosstat b\n",
    "    on b.id = (select id from tp_tramosstat where id_tptramosstat = a.tramosidsnew_t and tiem_creac < a.instant_date_t\n",
    "                    order by tiem_creac desc limit 1)) main\n",
    "    left join ts_equipos e\n",
    "    on main.eq_id = e.id_equipo\t\t\t\t\n",
    "    where main.nombre_tramo is not null\n",
    "    order by main.instant_date_t asc\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(tu_query_sql)\n",
    "\n",
    "    # Obtener los resultados en un DataFrame de pandas\n",
    "    resultados17 = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "    # Cerrar el cursor y la conexión\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Hacer lo que necesites con los resultados\n",
    "    print(resultados17.head())\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error al conectar a la base de datos PostgreSQL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87692726-bc7e-4368-b32a-9a3c2ae9a6d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ff35ec-047f-4c69-8ccd-d3b5d5165ffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados17.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dab1b61-2df8-4365-a40d-d5b0d817c6e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Guardarmos directamente el dfpandas en un csv en el BlobStorage\n",
    "Nota importante: Debes comentar la variable \"connection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1931775-096f-4956-b03f-3700643b48c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Convertir el DataFrame de Pandas a CSV\n",
    "#csv_data = resultados15.to_csv(\"datos_raw_shougang_equipo199.csv\", index=False)\n",
    "csv_data = resultados17.to_csv(index=False)\n",
    "\n",
    "# Obtener conection,de Azure DataLake,interfaz de Azure (Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL \n",
    "# CODIGO EN GIT,GITHUB)\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Nombre del contenedor y archivo en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "blob_name = \"datos_raw_shougang_equipo201.csv\"\n",
    "\n",
    "# Subir el archivo CSV al Blob Storage\n",
    "# with open(\"datos_raw_shougang_equipo61.csv\", \"rb\") as data:\n",
    "#     blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "#     blob_client.upload_blob(data)\n",
    "\n",
    "# Subir el archivo CSV al Blob Storage\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "blob_client.upload_blob(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f336d3d-f0e0-4423-b0e5-831dbbcb1030",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unir todos los csv(datalake-equiposacarreo), Se leen varios dfs(csv) en un solo df\n",
    "Nota importante: Debes comentar la variable \"conection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5971fe-516f-423c-a710-06ba58861b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Obtener conection,de Azure DataLake,interfaz de Azure (Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB)\n",
    "# connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# Crear el cliente del servicio Blob\n",
    "# blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Nombre del contenedor en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada archivo CSV\n",
    "dfs = []\n",
    "\n",
    "# Nombres de los archivos CSV\n",
    "csv_files = [\"datos_raw_shougang_equipo61.csv\",\"datos_raw_shougang_equipo65.csv\",\"datos_raw_shougang_equipo68.csv\",\"datos_raw_shougang_equipo126.csv\",\"datos_raw_shougang_equipo199.csv\",\"datos_raw_shougang_equipo199.csv\",\n",
    "             \"datos_raw_shougang_equipo201.csv\",\"datos_raw_shougang_equipo205.csv\"]\n",
    "\n",
    "# Leer cada archivo CSV y guardarlos en DataFrames individuales\n",
    "for csv_file in csv_files:\n",
    "    # Descargar el archivo CSV del Blob Storage\n",
    "    with open(csv_file, \"wb\") as my_blob:\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_file)\n",
    "        blob_data = blob_client.download_blob()\n",
    "        blob_data.readinto(my_blob)\n",
    "    \n",
    "    # Leer el archivo CSV en un DataFrame de Pandas\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Agregar el DataFrame a la lista\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo al final\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Visualizar el DataFrame resultante\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4013045c-f6c0-48e2-bda4-8cdf56b43d43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Guardar el csv consolidado C4M (se guarda 1 solo archivo de csv)\n",
    "Nota importante: Debes comentar la variable \"conection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbfa8984-f4b1-4391-95bd-9bee20c21395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Convertir el DataFrame de Pandas a CSV\n",
    "result_df.to_csv(\"datos_raw_shougang_c4m.csv\", index=False)\n",
    "\n",
    "# Obtener conection,de Azure DataLake,interfaz de Azure (Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB)\n",
    "# connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# Conectar al Blob Storage de Azure\n",
    "blob_service_client = BlobServiceClient.from_connection_string(conection_string)\n",
    "\n",
    "# Nombre del contenedor y archivo en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "blob_name = \"datos_raw_shougang_c4m.csv\"\n",
    "\n",
    "# Subir el archivo CSV al Blob Storage\n",
    "with open(\"datos_raw_shougang_c4m.csv\", \"rb\") as data:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab4151fc-2e02-49ce-9a32-ea4c8f0ad667",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Leer csv consolidado c4m (se lee solo 1 CSV)\n",
    "Nota importante: Debes comentar la variable \"conection_string\" Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3a6b2b-a765-4615-a84a-3d673c2363e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Obtener la conexión al Blob Storage de Azure\n",
    "#connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# Crear el cliente del servicio Blob\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Nombre del contenedor y archivo en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_c4m/operacion_shougang/\"\n",
    "blob_name = \"datos_raw_shougang_c4m.csv\"\n",
    "\n",
    "# Descargar el archivo CSV del Blob Storage\n",
    "with open(\"datos_raw_shougang_c4m.csv\", \"wb\") as my_blob:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    blob_data = blob_client.download_blob()\n",
    "    blob_data.readinto(my_blob)\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame de Pandas\n",
    "datos = pd.read_csv(\"datos_raw_shougang_c4m.csv\")\n",
    "\n",
    "# Visualizar el DataFrame\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8299c8-ec0e-41e1-a1f0-8a86c3309c55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados_equipos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9731a8-eff7-4977-a3a4-7231abd75cb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizar left join\n",
    "merged_df = pd.merge(datos, resultados_equipos[['id_equipo','nombre']], left_on='eq_id', right_on='id_equipo', how='left')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ae7a090-8a43-4900-a347-5ecd0fc26ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extraer datos de TP_EQUIPOS para extraer el Id_equipo y cruzar C4M y H4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe59738b-7c8f-451c-af1e-914f4e85106f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Información de la conexión a PostgreSQL\n",
    "host = \"srv-postgres-d4m.postgres.database.azure.com\"\n",
    "database = \"ControlSenseDB\"\n",
    "user = \"administrador\"\n",
    "password = \"Protobuffers2024\"\n",
    "\n",
    "# Establecer la conexión a la base de datos\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "\n",
    "    # Crear un cursor para ejecutar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Establecer la zona horaria antes de ejecutar la consulta\n",
    "    time_zone_query = \"SET TIME ZONE 'America/Lima';\"\n",
    "    cursor.execute(time_zone_query)\n",
    "\n",
    "    # Tu consulta SQL equipos de acarreo a considerar en c4mequipos y datacamion: 61,63,64,65,68,69,71,74,103,122,125,126,198,199,201,204,205\n",
    "    # equipos acarreo disponibles en telemetria h4m(2dias) : 61,65,68,126,199,201,*204*,205\n",
    "    tu_query_sql = '''\n",
    "    select * from ts_equipos\n",
    "    --where nombre = 'FC89'\n",
    "    '''\n",
    "    # Ejecutar la consulta\n",
    "    cursor.execute(tu_query_sql)\n",
    "\n",
    "    # Obtener los resultados en un DataFrame de pandas\n",
    "    resultados_equipos = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "    # Cerrar el cursor y la conexión\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Hacer lo que necesites con los resultados\n",
    "    print(resultados_equipos.head())\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error al conectar a la base de datos PostgreSQL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e780dbc9-cfed-4a43-af89-2d6fdbca7553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados_equipos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c04c65b-6aba-43c8-9c2a-d49f4a3952fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultados.loc[resultados['id_equipo'] == 199]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bfa3024-0c76-42cb-a0e4-84cdc53711bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Datos de H4M - Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7399cc37-2451-4191-b5d7-792ffa45c84e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d738c610-e1e6-4720-b42c-b55f5820f380",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Obtener conection,de Azure DataLake,interfaz de Azure (Claves de acceso: Key1) (Debes comentar esta variable Si NO deja hacer COMMIT DEL #CODIGO EN GIT,GITHUB)\n",
    "connection_string = 'DefaultEndpointsProtocol=https;AccountName=datalakemlopsd4m;AccountKey=iWT8t74/#XlqcqoR03keDVtFZPzr0PB9zDffMPaLWMUBIAjUww8uYAVkc9xRkcBtvTmUHKBvd1sB3+ASt6mGgcQ==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "# Crear el cliente del servicio Blob\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Nombre del contenedor en el Blob Storage\n",
    "container_name = \"raw/proyectocongestion_raw/fuentedatos_h4m/operacion_shougang/equipo_fc89\"\n",
    "\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada archivo CSV\n",
    "dfs = []\n",
    "\n",
    "# Nombres de los archivos CSV\n",
    "csv_files = [\"fc89_allvar_00_7_1marzo.csv\",\"fc89_allvar_7_13_1marzo.csv\",\"fc89_allvar_13_19_1marzo.csv\",\"fc89_allvar_19_2359_1marzo.csv\"]\n",
    "\n",
    "# Leer cada archivo CSV y guardarlos en DataFrames individuales\n",
    "for csv_file in csv_files:\n",
    "    # Descargar el archivo CSV del Blob Storage\n",
    "    with open(csv_file, \"wb\") as my_blob:\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_file)\n",
    "        blob_data = blob_client.download_blob()\n",
    "        blob_data.readinto(my_blob)\n",
    "    \n",
    "    # Leer el archivo CSV en un DataFrame de Pandas\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Agregar el DataFrame a la lista\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo al final\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Visualizar el DataFrame resultante\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d734246-b3c7-457f-a77e-bab73720af71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.ingestion_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
