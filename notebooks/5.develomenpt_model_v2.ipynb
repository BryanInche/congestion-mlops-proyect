{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e0fdb0-bcc8-4781-93c0-6419d04d015c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Modelo Serie Temporal (Ajustado a Aprendizaje Supervisado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20f19ff-ade5-428a-a9f0-3d15b64e2e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tratamiento de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Graficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "#Configuramos pandas para que lanze valores con una precision de hasta 6 decimales\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "import sys\n",
    "# Establecer la opción de impresión para mostrar el array completo\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2903b50a-3502-44ae-8af0-10c9639b904c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Leemos los datos de PROCEESED la tabla Delta \n",
    "# df_delta2 = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/proyectocongestion_presentation/tablacaracteristicas_congestion_tabladelta_v3\")\n",
    "df_delta2 = spark.read.format(\"delta\").load(\"/mnt/datalakemlopsd4m/presentation/proyectocongestion_presentation/data_congestion_serietemp_balanceada\")\n",
    "datos_cong_balanceado = df_delta2.toPandas()\n",
    "datos_cong_balanceado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc4252b-efcb-4516-b8f7-ddb650f511f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ajuste de los datos para problemas de Time Series(Establecer la Frecuencia )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87fb7fc1-670a-4e72-935f-5283bb5625bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #2. Ajuste de los datos a Series Temporales\n",
    "# #2.1 Se establece la columna 'Time' como el índice del DataFrame \"datos\"\n",
    "datos_cong_balanceado = datos_cong_balanceado.set_index('instant_date_t')\n",
    "\n",
    "# #2.3 Ordenamos el dataset de forma ascendente segun el datetime\n",
    "datos_cong_balanceado.sort_index(inplace=True)\n",
    "\n",
    "#2.4 Identificamos la periocidad de la serie temporal\n",
    "df_time_diffs = datos_cong_balanceado.index.to_series().diff().dt.total_seconds()\n",
    "\n",
    "# # 2.4.1 Contar cuántas diferencias de tiempo tienen cada valor específico\n",
    "# diferencias_frecuencias = df_time_diffs.value_counts().sort_index()\n",
    "# # 2.4.2 Mostrar los recuentos\n",
    "# print(diferencias_frecuencias)\n",
    "\n",
    "#3. Eliminamos o Filtramos las filas donde la diferencia es distinta de cero (Con ello eliminamos las filas o registros de fechas duplicadas)\n",
    "#datos_cong_balanceado = datos_cong_balanceado[df_time_diffs != 0]\n",
    "datos_cong_balanceado = datos_cong_balanceado[df_time_diffs == 6]   # Solo nos quedamos con frecuencia que estas considerando\n",
    "\n",
    "#4. # Reinterpolar el dataset con una periosidad en especifico\n",
    "# 'S' o 'seg' : Segundo, 'T' o 'min': Minuto,'H' o 'Hr': Hora,'D': Día,'W': Semana,'M': Mes,'Q': Trimestre,'Y': Año\n",
    "#datos_cong_balanceado = datos_cong_balanceado.asfreq(freq='5T', method='bfill')\n",
    "datos_cong_balanceado = datos_cong_balanceado.asfreq(freq='6s', method='bfill')\n",
    "\n",
    "# Filtramos los valores de x, y, z que son 0s\n",
    "mask = (datos_cong_balanceado['x'] == 0) & (datos_cong_balanceado['y'] == 0) & (datos_cong_balanceado['z'] == 0)\n",
    "datos_cong_balanceado = datos_cong_balanceado[~mask]\n",
    "\n",
    "datos_cong_balanceado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "798117b5-3a94-49d7-9cfa-c95404326c3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_cong_balanceado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9755c44-c70a-465a-849b-7473409b1668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#datos_df = datos_cong_balanceado['congestion']  # Modelo Univariado\n",
    "\n",
    "\n",
    "#En Multivariado, poner la variable a predecir, en la columna final\n",
    "# c_variables = ['id_equipo','id_worker','id_path','n_sat','isload_t','marcha_t','precisiongps_t','x','y','z','direccion_t'\n",
    "# ,'speed_t','pitch_t','roll_t','segment_angle_t','tonelaje_t','fuel_rate_t','combustibleint_t'\n",
    "# ,'LCKUP_SLIP','BRK/AIR_PRES','RTF_LTF_BRKTEMP','RTR_LTR_BRKTEMP','RT_F_BRK_TEMP','RT_R_BRK_TEMP','LT_R_BRK_TEMP','SERV_BRK_STAT','Tire_Press_N4','Tire_Press_N6','Hourmeter_MSPU','Direction','congestion']\n",
    "\n",
    "c_variables = ['x','speed_t','congestion']\n",
    "\n",
    "\n",
    "datos_df = datos_cong_balanceado[c_variables]  # Modelo Multivariado\n",
    "datos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a815846d-6b60-4d49-9d67-f5df0881e1bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.4 Identificamos la periocidad de la serie temporal\n",
    "df_time_diffs = datos_df.index.to_series().diff().dt.total_seconds()\n",
    "\n",
    "# # 2.4.1 Contar cuántas diferencias de tiempo tienen cada valor específico\n",
    "diferencias_frecuencias = df_time_diffs.value_counts().sort_index()\n",
    "# # 2.4.2 Mostrar los recuentos\n",
    "print(diferencias_frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0628ab-4fc1-42f5-aee3-0cad2f188b59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_df.index.min(), datos_df.index.max(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5293e9-1c53-42df-a441-46a4be8896e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#datos_df.index.to_series().diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e002cd0-ce0a-43bd-b0f8-ec9b8ab42c3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_lags(df):\n",
    "    #Hacemos un diccionario con los valores del TARGET, para completar luego\n",
    "    target_map = df['congestion'].to_dict()\n",
    "\n",
    "    df['lag1'] = (df.index - pd.Timedelta('1days')).map(target_map)  #Retroceso 1, en este caso (3600*24  1 dia en segundos)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45f7f2d-560b-4766-a6b4-7f052147e2db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Escalar/Normalizar los datos \n",
    "- Es requerido para garantizar que todas las caracteristicas se encuentren en el mismo rango\n",
    "de valores, lo que facilita el entrenamiento del Modelo y las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5392c4b-62ca-4678-88ed-868b425ee1bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def escalar_por_variables_especificas(dataframe, variables_a_escalar, medias, desviaciones_estandar):\n",
    "  \"\"\"\n",
    "  Escala solo las variables especificadas en un dataframe de forma independiente.\n",
    "\n",
    "  Args:\n",
    "    dataframe: El dataframe que se va a escalar.\n",
    "    variables_a_escalar: Una lista que contiene los nombres de las variables que se van a escalar.\n",
    "    medias: Un diccionario que contiene la media de cada variable.\n",
    "    desviaciones_estandar: Un diccionario que contiene la desviación estándar de cada variable.\n",
    "\n",
    "  Returns:\n",
    "    El dataframe escalado.\n",
    "  \"\"\"\n",
    "  dataframe_escalado = dataframe.copy()\n",
    "  for columna in dataframe.columns:\n",
    "    if columna in variables_a_escalar:\n",
    "      dataframe_escalado[columna] = (dataframe[columna] - medias[columna]) / desviaciones_estandar[columna]\n",
    "  return dataframe_escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1fe97d-b140-42bc-b3e4-bd01fd30801b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Definir la lista de variables a escalar\n",
    "variables_a_escalar = [\"x\", \"speed_t\"]  # Reemplazar con los nombres reales de las columnas\n",
    "\n",
    "# Calcular medias y desviaciones estándar para todas las variables\n",
    "medias = datos_df.mean()\n",
    "desviaciones_estandar = datos_df.std()\n",
    "\n",
    "# Escalar solo las variables especificadas en el dataframe train\n",
    "df_escalado = escalar_por_variables_especificas(datos_df, variables_a_escalar, medias, desviaciones_estandar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8f366c-8df1-411e-a608-5c852089d975",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Generacion del DataSet Supervisado\n",
    "- Función de Python llamada series_to_supervised()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e174efa-82c3-4272-a984-001c1e4d1edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Función de Python llamada series_to_supervised() que toma una serie temporal univariada o multivariada y la encuadra como un #conjunto de datos de aprendizaje supervisado.\n",
    "import pandas as pd\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Secuencia de observaciones como una lista o matriz NumPy 2D.\n",
    "    n_in : número de observaciones de retraso como entrada ( X ). Los valores pueden estar entre [1..len(data)] Opcional. El valor predeterminado es 1.\n",
    "    n_out : Número de observaciones como salida ( y ). Los valores pueden estar entre [0..len(data)-1]. Opcional. El valor predeterminado es 1.\n",
    "    dropnan : valor booleano para eliminar o no filas con valores NaN. Opcional. El valor predeterminado es Verdadero.\n",
    "\n",
    "    Returns:\n",
    "    Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))    # Aplicas los pasos hacia atras \n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))   # Aplicas los pasos hacia adelante\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)    #Eliminamos las filas Nulas de nuestro df generado(Porque no sirven en la serie temporal)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d2c7fa-b12a-4407-9288-62990a6d633a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear los datasets de entrenamiento, prueba y validación y verificar sus tamaños\n",
    "numero_pasos_atras = 2  # Hiperparámetro\n",
    "numero_pasos_futuro = 1   # Modelo uni-step igual a 1 paso,   MUltivariado seria mayor a 1 paso al futuro\n",
    "\n",
    "# tr_s = series_to_supervised(tr.values, numero_pasos_atras , numero_pasos_futuro)\n",
    "# vl_s = series_to_supervised(vl.values, numero_pasos_atras, numero_pasos_futuro)\n",
    "# ts_s = series_to_supervised(ts.values, numero_pasos_atras, numero_pasos_futuro)\n",
    "\n",
    "datos_df_s = series_to_supervised(datos_df.values, numero_pasos_atras, numero_pasos_futuro)\n",
    "\n",
    "# Asignar nombres originales a las columnas\n",
    "original_columns = datos_df.columns\n",
    "\n",
    "new_columns = []\n",
    "for i in range(numero_pasos_atras, 0, -1):\n",
    "    new_columns += [f'{col}(t-{i})' for col in original_columns]\n",
    "for i in range(0, numero_pasos_futuro):\n",
    "    if i == 0:\n",
    "        new_columns += [f'{col}(t)' for col in original_columns]\n",
    "    else:\n",
    "        new_columns += [f'{col}(t+{i})' for col in original_columns]\n",
    "\n",
    "datos_df_s.columns = new_columns\n",
    "datos_df_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8635927f-febc-4f9f-aded-784882defae8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Solo nos quedamos en la ultima columna con la columna a predecir (Las demas que estan en el tiempo T se eliminan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d5eb5e-172d-44fe-9f94-2591a25bbf95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a8ba5f-d69f-401d-87dc-05b06e21164a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Solo nos quedariamos con la variable congestion(t)  como target\n",
    "columns_to_drop = ['x(t)', 'speed_t(t)']\n",
    "datos_df_s.drop(columns=columns_to_drop, inplace=True)\n",
    "datos_df_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd49b759-7e43-4eb7-a29e-af4e3f53a58e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Separamos los datos en Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4733ed0c-8e73-4399-afa6-14b7a55beef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Funcion para generar las particiones, siguiendo la secuencia de las series temporales\n",
    "def train_val_test_split(serie, tr_size =0.8, vl_size=0.1, ts_size=0.1):\n",
    "    # Definir nunmero de datos en cada subserie\n",
    "    N = serie.shape[0]   #Serie seria el df, con las columnas que intervendran en el Modelo\n",
    "    Ntrain = int(tr_size*N)  # Numero de datos de entrenamiento\n",
    "    Nval = int(vl_size*N)\n",
    "    Ntest = int(ts_size*N)\n",
    "\n",
    "    #Realizar la particion\n",
    "    train = serie[0:Ntrain]\n",
    "    val = serie[Ntrain:Nval+Ntrain]\n",
    "    test = serie[Nval+Ntrain:]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# Prueba de la funcion \n",
    "tr, vl, ts  = train_val_test_split(datos_df_s)\n",
    "\n",
    "print(f'Shape del Set de Entrenamiento, {tr.shape}')\n",
    "print(f'Shape del Set de Validacion, {vl.shape}')\n",
    "print(f'Shape del Set de Test, {ts.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3efa483d-64bd-498e-be0a-0e6c2c2ce69b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Separamos las X(Variables predictoras) e Y(variable target), para luego poner en X_train, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d56a24-b733-44af-a706-2f11ce3f4ef0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def separar_X_y(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "# Separar características y objetivo para cada conjunto\n",
    "X_train, y_train = separar_X_y(tr, 'congestion(t)')\n",
    "X_val, y_val = separar_X_y(vl, 'congestion(t)')\n",
    "X_test, y_test = separar_X_y(ts, 'congestion(t)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a90b31-90d8-4eb7-92ff-ca2c1a4466b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46768312-e957-4f0a-adc3-1041cc28a1ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "La red LSTM espera que los datos de entrada (X) se proporcionen con una estructura de matriz específica en forma de: [muestras, pasos de tiempo, características]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2689c3-36f2-4e6a-943a-be6b23e01cea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = X_train.shape[1]  # número de características predictoras\n",
    "# Función para reshape de X para LSTM\n",
    "numero_bloques = 2     # Bloques de datos (filas) atras que iremos pasando al Modelo LSTM\n",
    "\n",
    "def reshape_X_for_lstm(X, numero_bloques, n_features):\n",
    "    n_samples = X.shape[0] - numero_bloques - numero_pasos_futuro + 1\n",
    "    X_lstm = np.zeros((n_samples, numero_bloques, n_features))\n",
    "    for i in range(n_samples):\n",
    "        X_lstm[i] = X.iloc[i:i+numero_bloques, :].values\n",
    "    return X_lstm\n",
    "\n",
    "# Aplicar reshape para XS( X_train, X_val, X_test) para LSTM\n",
    "X_train_lstm = reshape_X_for_lstm(X_train, numero_bloques, n_features)\n",
    "X_val_lstm = reshape_X_for_lstm(X_val, numero_bloques, n_features)\n",
    "X_test_lstm = reshape_X_for_lstm(X_test, numero_bloques, n_features)\n",
    "\n",
    "# Función para reshape para Ys () para LSTM\n",
    "def reshape_y_for_lstm(y, numero_pasos_futuro):\n",
    "    return y.iloc[numero_bloques + numero_pasos_futuro - 1:].values.reshape(-1, numero_pasos_futuro, 1)\n",
    "\n",
    "# Aplicar reshape para y_train, y_val, y_test\n",
    "y_train_lstm = reshape_y_for_lstm(y_train, numero_pasos_futuro)\n",
    "y_val_lstm = reshape_y_for_lstm(y_val, numero_pasos_futuro)\n",
    "y_test_lstm = reshape_y_for_lstm(y_test, numero_pasos_futuro)\n",
    "\n",
    "# Verificar las formas después del reshape para LSTM\n",
    "print(\"Shape de X_train_lstm:\", X_train_lstm.shape)\n",
    "print(\"Shape de y_train_lstm:\", y_train_lstm.shape)\n",
    "print(\"Shape de X_val_lstm:\", X_val_lstm.shape)\n",
    "print(\"Shape de y_val_lstm:\", y_val_lstm.shape)\n",
    "print(\"Shape de X_test_lstm:\", X_test_lstm.shape)\n",
    "print(\"Shape de y_test_lstm:\", y_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443d3f7e-6834-43f9-a8fb-584750f3e610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d4e60d-89d8-4bef-8024-cdcfb76939ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "X_train_lstm[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f9a2fba-0926-4460-9086-10d2c5b954f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Metodos para pasar a formato de input para Redes Neuronales LSTM\n",
    "Ambos métodos pueden ser válidos dependiendo del contexto y la estructura de los datos.\n",
    "\n",
    "(282653, 2, 7), (282653, 1, 1) para entrenamiento:\n",
    "\n",
    "Aquí, cada muestra tiene 2 pasos de tiempo y 7 características para las variables predictoras, y un solo valor de salida. Esto es adecuado si estás modelando datos donde tienes múltiples pasos de tiempo y varias características.\n",
    "\n",
    "\n",
    "(282653, 14), (282653,) para entrenamiento:\n",
    "\n",
    "En este caso, todas las características se aplanan en una sola dimensión, lo que significa que tratas todas las características como una secuencia única de datos por muestra. Esto puede ser útil en algunos casos donde no necesitas modelar explícitamente múltiples pasos de tiempo, pero simplemente quieres predecir un solo valor en el siguiente paso de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1026a823-541b-409a-9c54-63bca774123f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656c74fe-6292-478a-8546-eadd2bec5a49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Construccion del Modelo\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fijar Valores parametros , para asegurar la reproducibilidad de los datos inicializados\n",
    "tf.random.set_seed(123) # Semilla(inicializacion de los parametros o pesos de misma manera)\n",
    "tf.config.experimental.enable_op_determinism() # Cada vez se ejecute de misma manera\n",
    "\n",
    "# El Modelo\n",
    "N_UNITS = 50 # Tamaño del estado oculto de la celda de Memoria de LSTM\n",
    "INPUT_SHAPE = (X_train_lstm.shape[1], X_train_lstm.shape[2]) #pasos atras x n features (feature)\n",
    "\n",
    "modelo = Sequential()\n",
    "modelo.add(LSTM(N_UNITS, input_shape=INPUT_SHAPE))\n",
    "modelo.add(Dense(numero_pasos_futuro, activation='sigmoid')) # linear: problema regresion, sigmoid:clasificacion binaria, softmax:multiclase\n",
    "\n",
    "#RSME: Para problemas de regresion, para tener errores en las mismas unidades de la variable target\n",
    "# def root_mean_squared_error(y_true,y_pred):\n",
    "#     rmse=tf.math.sqtr(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "#     return rmse\n",
    "\n",
    "# Optimizador para problema de regresion\n",
    "# optimizador = RMSprop(learning_rate=0.05)\n",
    "\n",
    "#Compilation\n",
    "\n",
    "\n",
    "# Compilar el modelo\n",
    "modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','AUC'])\n",
    "\n",
    "# Entrenamiento del Modelo\n",
    "EPOCHS = 50 # Numero de epocas de entrenamiento\n",
    "BATCH_SIZE = 256 # Numero de lotes que van ingresando al Modelo(Algoritmo)\n",
    "historia = modelo.fit(\n",
    "    x = X_train_lstm,\n",
    "    y = y_train_lstm,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (X_val_lstm, y_val_lstm),\n",
    "    verbose=2  #Imprime en pantalla con va el entrenamiento\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61edf098-8301-4fe7-bd6a-9041e3adde96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Graficar curvas de entrenamiento y validacion, para verificar que no existe overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79dfc040-528e-4e91-8c42-27902fea95cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(historia.history['loss'],label='Loss Train')\n",
    "plt.plot(historia.history['val_loss'],label='Loss Validation')\n",
    "plt.xlabel('Iteracion')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135e51a4-3b5b-4b0f-835f-dd30cba57ae5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(historia.history['accuracy'],label='Accuracy Train')\n",
    "plt.plot(historia.history['val_accuracy'],label='Accuracy Validation')\n",
    "plt.xlabel('Iteracion')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1af54371-4bfd-4e84-8bdb-ec0b5ff4f7bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pronostico Futuro Con Multivariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61314e3-faf5-4e3d-a38b-1fece63c6bde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datos_df.index.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ede055d-44ca-4603-8f22-bb68416c111a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "futuretesting=pd.date_range(datos_df.index.max() +  pd.Timedelta('6s'), datos_df.index.max() + pd.Timedelta(days=1) , freq='6s')\n",
    "futuretesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f616581-4dc8-4830-bd6c-b8a51968b25d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creature future dataframe\n",
    "future_df=pd.DataFrame(index=futuretesting)\n",
    "\n",
    "# #Se agrega la columna de FUTURE en el df pronostico com TRUE (para indicar que son el futuro)\n",
    "future_df['isFuture'] = True\n",
    "\n",
    "# #Se agrega la columna de FUTURE en el df Original con FALSE (para indicar no es futuro) \n",
    "datos_df['isFuture'] = False\n",
    "\n",
    "#Se hace una concatenacion para agregar las filas de future_df al df_original\n",
    "df_and_future = pd.concat([datos_df, future_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20abe81-64cf-48dd-8fc2-646db714a0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_and_future = add_lags(df_and_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fc7b92-9c34-49fc-94da-e6c787f163b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_and_future.tail(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089f1d2b-8c94-4d1f-8355-02ff4e2d2832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filtramos solo el Dataset Esqueleto para el Pronostico\n",
    "future_dates_pronostico = df_and_future.query('isFuture').copy()\n",
    "future_dates_pronostico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14dc74c2-2c32-4f4a-b77d-e85f01fdb078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "futuretesting - pd.Timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1487f7ef-5075-4321-b4b9-900bad031222",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identificamos las columnas a completar usando los datos del día anterior\n",
    "columns_to_fill = ['x', 'speed_t']\n",
    "\n",
    "# Rellenamos las columnas específicas con los valores del día anterior\n",
    "for col in columns_to_fill:\n",
    "    future_dates_pronostico[col] = df_and_future[col].loc[futuretesting - pd.Timedelta(days=1)].values\n",
    "\n",
    "# Las otras columnas se quedan con NaN o el valor que tenías en el DataFrame de predicción\n",
    "future_dates_pronostico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de58f769-43f9-404f-b5e2-258b4d9d16fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FEATURES = ['x','speed_t','lag1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3cf646-719a-4247-8658-5a5949c87d19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reshape_X_for_lstm(X, numero_bloques, n_features):\n",
    "    n_samples = X.shape[0] - numero_bloques - numero_pasos_futuro + 1\n",
    "    X_lstm = np.zeros((n_samples, numero_bloques, n_features))\n",
    "    for i in range(n_samples):\n",
    "        X_lstm[i] = X.iloc[i:i+numero_bloques, :].values\n",
    "    return X_lstm"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.develomenpt_model_v2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
